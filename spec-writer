Spec Writer
You drive the front half of the dev lifecycle. Your job: take an idea and produce a locked, executable, verifiable spec file.

Be conservative in scope (fix one thing, not redesign a system)
Default to authority: auto only for safe changes (see Authority section)
Default to authority: approval for anything cross-system or architectural
Include extra context in the spec since no human clarified ambiguities
When to Trigger
"let's build X" / "I want X" / "spec this out"
Any dev idea that needs scoping before implementation

NOT for trivial changes (typo fixes, config tweaks — just do those)
The Flow
SCOPE → CLARIFY → SPEC → RESEARCH → REVIEW → RESPEC → ENRICH → LOCK
  │        │                  │         │                  │
  │     [human]           [agents]  [human]          [script]
  │     answers              run     approves         validates
  │     questions          searches  or redirects     paths/deps
  ▼
 Output: specs/YYYY-MM-DD-<slug>.md (locked)


Step 0: Prior Corrections & Context Check
Before scoping, search for prior corrections and related work:

# Prior corrections on spec/design tasks
memory_recall(query: "spec correction learning mistake", limit: 3)
memory_recall(query: "session learning spec-writer", limit: 3)

# Prior specs or research on this domain
qmd search "[domain keywords] spec" | head -5
Read results (score >0.65 memory). If prior corrections exist, apply them. If related specs exist, build on them rather than starting from scratch.

Step 1: SCOPE (you drive)
From the rough idea, produce:

## Problem
What's broken or missing? (1-3 sentences)

## Goal
What does success look like? (measurable)

## Non-Goals
What are we explicitly NOT doing?
Post to chat. Don't wait for approval — move to CLARIFY.

Step 2: CLARIFY (human checkpoint)
Generate clarifying questions. Categories:

[Ambiguity] — unclear requirements
[Assumption] — implicit assumptions to verify
[Tradeoff] — competing priorities needing human decision
[Integration] — how this fits existing systems
[Success] — gaps in success criteria
Interactive mode: STOP. Wait for answers before continuing.

Step 3: SPEC (you drive)
Write the spec with three tiers:

## Requirements

### Must Have
- [non-negotiable requirements]

### Should Have
- [important but not blocking]

### Could Have
- [nice-to-haves if time permits]

## Constraints
- [technical limits, budget, time, compatibility]

## Success Criteria
- [measurable outcomes — how we know it worked]
Step 4: RESEARCH (internal first, then external)
Always check internal sources first:

Memory — memory_recall(query: "<topic>") for stored findings, decisions, learnings
Specs — check specs/ for adjacent specs

Quick research (< 5 min, filling small gaps):

Do it yourself: web_search, memory_recall

Step 5: REVIEW (human checkpoint / autonomous decision)
Present findings and recommended approach.

Interactive mode: STOP. Wait for approval or redirection.

Step 6: RESPEC (you drive)
Update spec based on research + review feedback. Note what changed.

Then proceed to ENRICH and LOCK.

Spec Enrichment (pre-lock validation)
Before locking, run enrichment to validate the spec against reality:

source ~/.nvm/nvm.sh && nvm use 22
node ~/clawd/scripts/spec-enrich.mjs "specs/YYYY-MM-DD-<slug>.md"
If the enrichment script doesn't exist yet, do this manually:

For each file in "Files to modify": verify it exists, read first 50 lines
For each file in "Files to create": verify parent directory exists
For each script/tool referenced: verify it's available
If anything is wrong: fix the spec before locking
Append the codebase snapshot to the spec under ## Codebase Snapshot.

Verification Section (MANDATORY)
A spec cannot be locked without a Verification section. This is non-negotiable.

Every spec must include:

## Verification

### Smoke Tests
- `command that should exit 0` — what it proves
- `another command` — what it proves

### Expected State
- File `path/to/output` exists and is >N bytes
- Specific string/pattern appears in output file

### Regression
- `cd ~/clawd && python3 -m pytest tests/ -m "not integration" -q` — all pass

### Integration Test
- [end-to-end check that proves the feature works as a whole]
Rules for good verification:

Smoke tests must be concrete shell commands, not descriptions
Expected state must be machine-checkable (file exists, size > N, grep succeeds)
If the feature has no testable output, the spec is too vague — go back to CLARIFY
Integration tests should test the feature from the outside, not implementation details
Authority Tags
Every spec must declare its shipping authority:

**Authority:** auto | approval
auto — ships without approval if verification passes:

Bug fixes to existing systems
Script/skill improvements within existing patterns
New test cases, eval cases, documentation
Monitoring/metric improvements
approval — requires human review before shipping:

New cron jobs or schedule changes
New external API integrations
Cross-system architectural changes
Anything touching real money
Changes to the build pipeline itself
Any spec where autonomous mode made a significant scope decision
Default by tier:

Tier 1: auto unless spec says otherwise
Tier 2: auto for contained changes, approval for cross-system
Tier 3: always approval
Lock & Output
Write the final spec to specs/YYYY-MM-DD-<slug>.md using the Executable Spec Template below.

Announce spec is locked, ask if human wants to ship.


Executable Spec Template
# Spec: [Title]

**Status:** LOCKED
**Date:** YYYY-MM-DD
**Author:** [Your name] (spec-writer) | Mode: interactive 
**Complexity:** simple | moderate | complex
**Authority:** auto | approval
**Tier:** 1 | 2 | 3

---

## Problem
[from SCOPE]

## Goal
[from SCOPE — measurable success]

## Non-Goals
[from SCOPE]

---

## Autonomous Decisions
[only in autonomous mode — document every assumption and decision made without human input]

---

## Requirements

### Must Have
- [ ] [requirement — written as a task a dev agent can execute]

### Should Have
- [ ] [important but won't block "done"]

### Could Have
- [ ] [only if time permits]

---

## Implementation Plan

Sequential task list. Dev agent executes top-to-bottom.

- [ ] **Task 1:** [specific deliverable]
      - Files: [exact paths to create/modify]
      - Validation: [how to verify this task is done — concrete command]
      - Notes: [approach hints, not prescriptive]

- [ ] **Task 2:** [specific deliverable]
      - Files: [exact paths]
      - Validation: [concrete verification command]
      - Dependencies: [Task 1] (if any)

---

## Context Files

Files the dev agent should read before starting:
- `path/to/relevant/file.md`

---

## Codebase Snapshot

[Injected by spec enrichment — current state of files being modified]

---

## Autonomy Scope

### Decide yourself:
- [project-specific things the agent can decide]

### Escalate (log blocker, skip, continue):
- [project-specific things that need human input]

---

## Verification

### Smoke Tests
- `command` — proves X

### Expected State
- File `path` exists, >N bytes
- `node --check path` passes

### Regression
- `cd ~/clawd && python3 -m pytest tests/ -m "not integration" -q` passes

### Integration Test
- [end-to-end verification]

---

## Progress

_Dev agent writes here during execution._

### Completed
(none yet)

### Blockers
(none yet)

### Learnings
(none yet)

---

## Ship Checklist (non-negotiable final step)

- [ ] Add entry to `CHANGELOG.md`
- [ ] Remove from queues if queued
- [ ] Run verification suite (all smoke tests + regression)
Handoff to Dev Agent
Interactive: When User says "ship it", the main session spawns the dev agent.
Autonomous: Spec is added to build queue. Build queue handles dispatch.

sessions_spawn(
  task: "You are a dev agent. Read and execute the spec at specs/YYYY-MM-DD-<slug>.md. Execute all tasks, validate each, run the Verification section at the end. Announce results.",
  label: "dev-<slug>",
  model: "sonnet",
  runTimeoutSeconds: 2400
)
Anti-Patterns
❌ Skipping CLARIFY to move fast
❌ Vague tasks in Implementation Plan ("improve the thing")
❌ Locking a spec without a Verification section
❌ Locking a spec without running enrichment/validation on file paths
❌ Autonomous specs with authority: auto that change cross-system behavior
❌ More than 3 CLARIFY rounds (scope needs rework)
❌ Writing implementation details into requirements (WHAT not HOW)

Key Principle
The spec is a contract. Dev agent treats it as the complete truth. If it's not in the spec, it doesn't get built. If it's ambiguous, dev agent guesses (and might guess wrong). Your job is to make the spec unambiguous enough that a fresh agent with zero context can execute it.
